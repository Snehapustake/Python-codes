Scenario 1: Data ValidationTask: Write a function validate_data(data) that checks if a list of dictionaries (e.g., [{"name": "Alice", "age": 30}, {"name": "Bob", "age": "25"}]) contains valid integer values for the "age" key. Return a list of invalid entries.
def validate_data(data):
    """
    Validates that 'age' field in each dictionary is a valid non-negative integer.
    Returns list of invalid entries.
    """
    invalid_entries = []   
    for entry in data:
        if 'age' not in entry:
            invalid_entries.append(entry)
            continue
            
        try:
            age = int(entry['age'])  # Convert to int
            if age < 0:
                invalid_entries.append(entry)
        except (ValueError, TypeError):
            invalid_entries.append(entry)  # [web:14]
    
    return invalid_entries
data = [
    {"name": "Alice", "age": 30},
    {"name": "Bob", "age": "25"},      # Invalid: string
    {"name": "Charlie", "age": 25},
    {"name": "David"},                 # Invalid: missing age
    {"name": "Eve", "age": -5},        # Invalid: negative
    {"name": "Frank", "age": None}     # Invalid: None
]

invalid = validate_data(data)
print("Invalid entries:", invalid)



Scenario 2: Logging DecoratorTask: Create a decorator @log_execution_time that logs the time taken to execute a function. Use it to log the runtime of a sample function calculate_sum(n) that returns the sum of numbers from 1 to n.
import time

def log_execution_time(func):
    def wrapper(*args, **kwargs):
        start_time = time.time()          # Record start time [web:2]
        result = func(*args, **kwargs)    # Execute the original function [web:2]
        end_time = time.time()            # Record end time [web:2]
        execution_time = end_time - start_time
        print(f"Function {func.__name__} took {execution_time:.6f} seconds")  # [web:2][web:4]
        return result
    return wrapper
@log_execution_time
def calculate_sum(n):
    total = 0
    for i in range(1, n + 1):
        total += i
    return total

ans = calculate_sum(1000000)
print("Sum:", ans)



Scenario 3: Missing Value Handling
Task: A dataset has missing values in the "income" column. Write code to:

1. Replace missing values with the median if the data is normally distributed.

2. Replace with the mode if skewed.
Use Pandas and a skewness threshold of 0.5.

import pandas as pd
income_skew = df["income"].skew()   # Uses pandas Series.skew [web:19]

if abs(income_skew) <= 0.5:
    median_val = df["income"].median()
    df["income"] = df["income"].fillna(median_val)  # [web:16][web:17][web:27]
else:
    mode_val = df["income"].mode()[0]
    df["income"] = df["income"].fillna(mode_val)    # [web:16][web:23][web:27]



Scenario 4: Text Pre-processing
Task: Clean a text column in a DataFrame by:

1. Converting to lowercase.

2. Removing special characters (e.g., !, @).

3. Tokenizing the text.

import pandas as pd
import re
import nltk
from nltk.tokenize import word_tokenize

# Assume df is your DataFrame and "text_col" is the target column

# Download NLTK data if needed (run once)
# nltk.download('punkt')

def clean_text(text):
    if pd.isna(text):
        return text
    text = text.lower()  # Convert to lowercase [web:29][web:33]
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters [web:33][web:34][web:39]
    tokens = word_tokenize(text)  # Tokenize into words [web:35][web:38]
    return tokens

df["cleaned_text"] = df["text_col"].apply(clean_text)  # Apply cleaning [web:29][web:35]




Scenario 5: Hyperparameter Tuning
Task: Use GridSearchCV to find the best max_depth (values: [3, 5, 7]) and n_estimators (values: [50, 100]) for a Random Forest classifier.

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Assume X_train, y_train are your feature matrix and target

# Define parameter grid
param_grid = {
    'max_depth': [3, 5, 7],
    'n_estimators': [50, 100]
}  # [web:45][web:48][web:52]

rf = RandomForestClassifier(random_state=42)  # [web:44]

grid_search = GridSearchCV(rf, param_grid, cv=5)  # [web:45][web:46][web:48]
grid_search.fit(X_train, y_train)  # [web:45][web:48]

# Best parameters and model
print("Best parameters:", grid_search.best_params_)  # [web:45][web:48]
best_rf = grid_search.best_estimator_  # [web:45][web:46]



Scenario 6: Custom Evaluation Metric
Task: Implement a custom metric weighted_accuracy where class 0 has a weight of 1 and class 1 has a weight of 2.

from sklearn.metrics import make_scorer, accuracy_score

def weighted_accuracy(y_true, y_pred):
    weight_0 = 1
    weight_1 = 2
    # Compute accuracy for each class
    acc_0 = ((y_true == 0) & (y_pred == 0)).sum() / (y_true == 0).sum()
    acc_1 = ((y_true == 1) & (y_pred == 1)).sum() / (y_true == 1).sum()
    # Weighted average
    total_weight = weight_0 + weight_1
    return (weight_0 * acc_0 + weight_1 * acc_1) / total_weight

# Create custom scorer
weighted_acc_scorer = make_scorer(weighted_accuracy)  # [web:59][web:61][web:67]

# Usage example in GridSearchCV:
# grid_search = GridSearchCV(model, params, scoring=weighted_acc_scorer, cv=5)



Scenario 7: Image Augmentation
Task: Use TensorFlow/Keras to create an image augmentation pipeline with random rotations (±20 degrees), horizontal flips, and zoom (0.2x).

import tensorflow as tf

# Modern approach using Keras preprocessing layers (recommended)
augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomRotation(factor=0.2/360, input_shape=(224, 224, 3)),  # ±20 degrees [web:78][web:81]
    tf.keras.layers.RandomFlip("horizontal"),  # Random horizontal flip [web:81][web:85]
    tf.keras.layers.RandomZoom(0.2)  # Zoom by ±0.2 [web:81][web:87]
])  # [web:81][web:85]

# Alternative: Legacy ImageDataGenerator
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=20,  # ±20 degrees [web:74][web:75][web:80]
    horizontal_flip=True,  # Horizontal flips [web:74][web:75]
    zoom_range=0.2  # Zoom 80-120% [web:74][web:75][web:80]
)



Scenario 8: Model Callbacks
Task: Implement an EarlyStopping callback that stops training if validation loss doesn’t improve for 3 epochs and restores the best weights.

from tensorflow.keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(
    monitor='val_loss',        
    patience=3,                
    restore_best_weights=True, 
    verbose=1                  
)



Scenario 9: Structured Response Generation
Task: Use the Gemini API to generate a response in JSON format for the query: "List 3 benefits of Python for data science." Handle cases where the response isn’t valid JSON.

import google.generativeai as genai
import json
import re

genai.configure(api_key="YOUR_API_KEY")

model = genai.GenerativeModel('gemini-1.5-flash')
response_schema = {
    "type": "object",
    "properties": {
        "benefits": {
            "type": "array",
            "items": {"type": "string"},
            "minItems": 3,
            "maxItems": 3
        }
    },
    "required": ["benefits"]
}  

response = model.generate_content(
    "List 3 benefits of Python for data science.",
    generation_config=genai.types.GenerationConfig(
        response_mime_type="application/json",
        response_schema=response_schema
    )
) 


try:
    result = json.loads(response.text)  # Parse JSON [web:105][web:108]
    print(result)
except json.JSONDecodeError:
  
   json_match = re.search(r'\{.*\}', response.text, re.DOTALL)
    if json_match:
        result = json.loads(json_match.group())
    else:
        result = {"benefits": ["Error parsing response"]}
    print(result)



Scenario 10: Summarization with Constraints
Task: Write a prompt to summarize a news article into 2 sentences. If the summary exceeds 50 words, truncate it to the nearest complete sentence.

def truncate_summary(summary, max_words=50):
    words = summary.split()
    if len(words) > max_words:
        sentences = summary.split('.')
        truncated = '.'.join(sentences[:-1])
        if len(truncated.split()) + len(sentences[-1].split()) <= max_words:
            truncated += '.' + sentences[-1]
        return truncated.strip()
    return summary
raw_summary = gemini_response  # From API call
final_summary = truncate_summary(raw_summary)
